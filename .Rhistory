theme_bw() +
theme(legend.position = "none") +
theme(axis.text.y = element_text(size = 18),
axis.text.x = element_text(size = 20),
axis.title.y = element_text(size = 22),
axis.title.x = element_text(size = 22),
strip.text = element_text(size = 24)) +
geom_label_repel(data = subset(plot_data),
aes(label = measure),
max.overlaps = 20,
direction = "y",
nudge_x = 0.2)
# zoomed in
plot_data %>%
ggplot(aes(x = group, y = loadings, group = measure, col = color))+
geom_point(size=3)+
geom_line(size=1)+
scale_colour_manual(values=cpal) +
scale_y_continuous(labels = function(x) format(x, digits = 1)) +
coord_cartesian(ylim = c(0, 0.005)) +
theme_bw() +
theme(legend.position = "none") +
theme(axis.text.y = element_text(size = 18),
axis.text.x = element_text(size = 20),
axis.title.y = element_text(size = 22),
axis.title.x = element_text(size = 22),
strip.text = element_text(size = 24)) +
geom_label_repel(data = subset(plot_data, group != "never_met"),
aes(label = measure),
max.overlaps = 20,
direction = "y",
nudge_x = 0.2)
plot_data$group <- factor(plot_data$group, levels = c("never_met","familiar"))
cpal <- c("#009E73","#D55E00")
# remove faceting
# zoomed out
plot_data %>%
ggplot(aes(x = group, y = loadings, group = measure, col = color))+
geom_point(size = 3)+
geom_line(size = 1)+
scale_colour_manual(values=cpal) +
coord_cartesian(ylim = c(0,0.33)) +
theme_bw() +
theme(legend.position = "none") +
theme(axis.text.y = element_text(size = 20),
axis.text.x = element_text(size = 20),
axis.title.y = element_text(size = 22),
axis.title.x = element_text(size = 22),
strip.text = element_text(size= 24))
# zoomed in
plot_data %>%
ggplot(aes(x = group, y = loadings, group = measure, col = color))+
geom_point(size=3)+
geom_line(size=1)+
scale_colour_manual(values=cpal) +
scale_y_continuous(labels = function(x) format(x, digits = 1)) +
coord_cartesian(ylim = c(0, 0.005)) +
theme_bw() +
theme(legend.position = "none") +
theme(axis.text.y = element_text(size = 18),
axis.text.x = element_text(size = 20),
axis.title.y = element_text(size = 22),
axis.title.x = element_text(size = 22),
strip.text = element_text(size = 24)) +
geom_label_repel(data = subset(plot_data, group != "never_met"),
aes(label = measure),
max.overlaps = 20,
direction = "y",
nudge_x = 0.2)
x <- readRDS("/Users/jkvrtilek/Downloads/fundfreq_contours_2024-09-29.RDS")
View(x)
library(tidyverse)
y <- x %>% select(-selec)
View(y)
z <- y %>% mutate(sum = rowsum())
z <- y %>% mutate(sum = rowsum(y))
z <- rowSums(y)
View(y)
y <- y %>% select(-sound.files)
z <- rowSums(y)
sum(z, na.rm = TRUE)
z <- rowSums(y, na.rm = TRUE)
sum(z, na.rm = TRUE)
# read in fund freq data
ff <- readRDS("ds_fundfreq_summary.RDS")
cat <- list.files(path = "/Users/jkvrtilek/Desktop/OSU/PhD/Ch1/rand_spec_fig/cerce")
cat <- list.files(path = "/Users/jkvrtilek/Desktop/OSU/PhD/Ch1/rand_spec_fig/cat")
cerce <- list.files(path = "/Users/jkvrtilek/Desktop/OSU/PhD/Ch1/rand_spec_fig/cerce")
vamp <-  list.files(path = "/Users/jkvrtilek/Desktop/OSU/PhD/Ch1/rand_spec_fig/vampirella")
alicia <-  list.files(path = "/Users/jkvrtilek/Desktop/OSU/PhD/Ch1/rand_spec_fig/alicia")
files <- c(cat,cerce,vamp,alicia)
f <- as.data.frame(files)
View(f)
write.csv(f, file = "/Users/jkvrtilek/Desktop/spectrogram_fig_files.csv")
load("/Users/jkvrtilek/Desktop/OSU/PhD/GitHub/call-convergence-2025/05_fit_models/model_fit_workspace_2024-10-09_1415.Rdata")
raw <- read.csv("/Users/jkvrtilek/Desktop/OSU/PhD/GitHub/_LFS/call-convergence-2025/vampire_call_measures_transformed.csv")
# tidy data and add sample sizes
d <-
raw %>%
separate(sound.files, into=c('ds','date', 'bat', 'file', 'call'), sep="_", remove = FALSE) %>%
group_by(bat) %>%
mutate(sample.size= n()) %>%
ungroup()
# load packages
library(tidyverse)
# tidy data and add sample sizes
d <-
raw %>%
separate(sound.files, into=c('ds','date', 'bat', 'file', 'call'), sep="_", remove = FALSE) %>%
group_by(bat) %>%
mutate(sample.size= n()) %>%
ungroup()
n(unique(d$bat))
unique(d$bat)
# I chose this sample size based on the comparing accuracy per bat with and without cross-validation
# bats with more than 80 calls have correct classification rates similar to their cross-validated classification rates
# see plot "accuracy-with-and-without-cross-validation-5.pdf"
min.sample.size <- 80
# filter calls by min sample size
d2 <-
d %>%
filter(sample.size >= min.sample.size)
unique(d2$bat)
arrange(unique(d2$bat))
sort(unique(d2$bat))
library(tidyverse)
library(plyr)
library(ggrepel)
library(geomtextpath)
library(ggh4x)
# get LD1s from familiar groups----
# zoo
fam_zoo <- read.csv("/Users/jkvrtilek/Desktop/OSU/PhD/GitHub/voice-vs-shape/familiar-dfa-loadings-zoo.csv") %>%
mutate(zoo = abs(LD1)) %>%
select(X, zoo)
# 2016
fam16 <- read.csv("/Users/jkvrtilek/Desktop/OSU/PhD/GitHub/voice-vs-shape/familiar-dfa-loadings-2016.csv") %>%
mutate(b16 = abs(LD1)) %>%
select(X, b16)
# 2019
fam19 <- read.csv("/Users/jkvrtilek/Desktop/OSU/PhD/GitHub/voice-vs-shape/familiar-dfa-loadings-2019.csv") %>%
mutate(b19 = abs(LD1)) %>%
select(X, b19)
# get average loading percentage for each variable
fam <- join_all(list(fam_zoo, fam16, fam19), by='X', type='left')
familiar <- fam %>%
gather(origin, value, -X) %>% spread(X, value)
# get LD1s from never-met groups----
raw <- read.csv("/Users/jkvrtilek/Desktop/OSU/PhD/GitHub/voice-vs-shape/never-met-dfa-loadings_1000.csv")
never_met <- raw %>%
gather(rep, value, -X) %>% spread(X, value)
# make plot for each variable----
for (i in 2:36) {
q <- quantile(never_met[,i], probs = c(0.025,0.975))
ci <- data.frame(quantile=names(q), values = unname(q))
print(
ggplot() +
geom_histogram(data = never_met, aes(x = never_met[,i])) +
geom_point(data = familiar, aes(x = familiar[,i], y = 0), color = "red", size = 5) +
geom_vline(data = ci, aes(xintercept = values, color = "red"), show.legend = F) +
xlab(paste(colnames(never_met)[i], colnames(familiar)[i], sep = "-"))
)
}
View(fam_zoo)
library(diffr)
diffr("/Users/jkvrtilek/Desktop/OSU/PhD/GitHub/call-convergence-2025/06_similarity_over_time01.R","/Users/jkvrtilek/Downloads/06_test_convergence_over_time01.R")
diffr("/Users/jkvrtilek/Desktop/OSU/PhD/GitHub/call-convergence/timeline_analysis/batvsgroup_dyadic6.R","/Users/jkvrtilek/Downloads/06_test_convergence_over_time01.R")
library(MASS)
library(tidyverse)
library(MASS)
library(tidyverse)
setwd(dirname(file.choose()))
#* call/recording data - large file, download from Figshare ----
raw <- read.csv("vampire_call_measures_transformed.csv")
setwd(dirname(file.choose()))
# tidy data, filter for 2016-17 bats, and add sample sizes
d <-
raw %>%
separate(sound.files, into=c('ds','date', 'bat', 'file', 'call'), sep="_", remove = FALSE) %>%
separate(date, into=c("year","month","day"), sep = "-", remove = FALSE) %>%
filter(year == "2016" | year == "2017") %>%
group_by(bat) %>%
mutate(sample.size= n()) %>%
ungroup()
min.sample.size <- 80
#* capture sites ----
capture.site <- read.csv("sex_age02.csv") %>%
dplyr::select(bat, capture.site) %>%
distinct
# filter calls by min sample size and capture site
d2 <- d %>%
filter(sample.size >= min.sample.size) %>%
left_join(capture.site, by = "bat") %>%
filter(capture.site == "tole" | capture.site == "las.pavas")
# get list of relevant bats
bats <- d2 %>%
filter(date > "2016-09-01") %>%
dplyr::select(bat, capture.site) %>%
distinct()
#* first contact data from 2016 ----
fc16 <- read_csv("first_contact_events2016.csv")
# make df of 2016 first contact dates
cont_date16 <- fc16 %>%
mutate(time = as.character(time)) %>%
separate(col = time, into = c("date", "time"), sep = " ") %>%
distinct(date) %>%
mutate(date = as.Date(date, "%Y-%m-%d"))
# Feb 7 is a typo for Jul 2
# Jul 7 and Aug 26 carry over from introductions on Jul 6 and Aug 25
# legitimate dates: 2016-07-02, 2016-07-06, 2016-08-25
smallcage <- cont_date16$date[1]
largecage <- cont_date16$date[5]
# make recording dates figure ----
# prep data frame
rec_dates <- d2 %>%
dplyr::select(sound.files, date, bat, capture.site) %>%
filter(bat %in% bats$bat) %>%
mutate(date = as.Date(date, "%Y-%m-%d")) %>%
mutate(bat = factor(bat, levels = c("eve","ola","ivy","cat","six","dcd","cs","ccs","scs","rc","ccc","r"))) %>%
mutate(CaptureSite = case_when(capture.site == "las.pavas" ~ "Las Pavas",
capture.site == "tole" ~ "Tole"))
# make labels
labels_lp <- rec_dates %>%
group_by(bat) %>%
arrange(desc(date)) %>%
slice(1L) %>%
filter(capture.site == "las.pavas")
labels_tole <- rec_dates %>%
group_by(bat) %>%
arrange(desc(date)) %>%
slice(1L) %>%
filter(capture.site == "tole")
# make plot
p <- rec_dates %>%
ggplot(aes(x=date, y=bat, group=bat)) +
facet_wrap(~CaptureSite, scales = "free_y", ncol = 1) +
geom_point(aes(color = capture.site)) +
geom_line() +
scale_x_date(date_labels = "%b %Y", breaks = "3 months") +
scale_color_manual(values = c("#0f2d59","#3eb7c7")) +
guides(color=guide_legend("Bat Origin")) +
geom_vline(xintercept = as.numeric(smallcage), color = "red", linetype = "dashed") +
geom_vline(xintercept = as.numeric(largecage), color = "red") +
ylab("individual bats") +
xlab("recording dates") +
theme_bw() +
theme(axis.text.y=element_blank(),
axis.ticks.y=element_blank(),
legend.position = "none",
plot.title = element_text(hjust = 0.5, size = 24),
axis.text.x = element_text(size = 16),
axis.title.x = element_text(size = 20),
axis.title.y = element_text(size = 20),
strip.text.x = element_text(size = 16)) +
geom_text(data = labels_lp, aes(label = bat), nudge_x = +15) +
geom_text(data = labels_tole, aes(label = bat), nudge_x = +15)
p
#* make functions ----
# get mean and 95% CI of values x via bootstrapping
boot_ci <- function(x, perms=5000, bca=F) {
get_mean <- function(x, d) {
return(mean(x[d]))
}
x <- as.vector(na.omit(x))
mean <- mean(x)
if(bca){
boot <- boot.ci(boot(data=x,
statistic=get_mean,
R=perms,
parallel = "multicore",
ncpus = 4),
type="bca")
low <- boot$bca[1,4]
high <- boot$bca[1,5]
}else{
boot <- boot.ci(boot(data=x,
statistic=get_mean,
R=perms,
parallel = "multicore",
ncpus = 4),
type="perc")
low <- boot$perc[1,4]
high <- boot$perc[1,5]
}
c(low=low,mean=mean,high=high, N=round(length(x)))
}
# function to get mean and 95% CI via bootstrapping of values y within grouping variable x
boot_ci2 <- function(d=d, y=d$y, x=d$x, perms=5000, bca=F){
df <- data.frame(effect=unique(x))
df$low <- NA
df$mean <- NA
df$high <- NA
df$n.obs <- NA
for (i in 1:nrow(df)) {
ys <- y[which(x==df$effect[i])]
if (length(ys)>1 & var(ys)>0 ){
b <- boot_ci(y[which(x==df$effect[i])], perms=perms, bca=bca)
df$low[i] <- b[1]
df$mean[i] <- b[2]
df$high[i] <- b[3]
df$n.obs[i] <- b[4]
}else{
df$low[i] <- min(ys)
df$mean[i] <- mean(ys)
df$high[i] <- max(ys)
df$n.obs[i] <- length(ys)
}
}
df
}
#* dyadic DFA between Las Pavas bats and pre-intro Tole bats ----
# prep dataframe
d3 <- d2 %>%
filter(bat %in% bats$bat) %>%
mutate(
group = case_when(capture.site == "las.pavas" ~ "lp",
capture.site == "tole" & date < "2016-08-01" ~ "tole1",
capture.site == "tole" & date > "2016-08-01" ~ "tole2")
)
# make dataframe for each group
# tole pre-intro bats
tole_pre <- d3 %>%
filter(group == "tole1") %>%
mutate(individual = bat) %>%
dplyr::select(group, individual, duration:segments)
# tole post-intro bats
tole_post <- d3 %>%
filter(group == "tole2") %>%
mutate(individual = bat) %>%
dplyr::select(group, individual, duration:segments)
# las pavas bats
lp <- d3 %>%
filter(group == "lp") %>%
mutate(individual = bat) %>%
dplyr::select(group, individual, duration:segments)
# get all PRE-INTRO dyadic distances
pre <- rbind(tole_pre,lp)
# classify calls to bat using a single dfa without cross validation
dfa_pre <- lda(individual ~
duration+
meanfreq+
sd+
freq.median+
freq.Q25+
freq.Q75+
freq.IQR+
time.median+
time.Q25+
time.Q75+
time.IQR+
skew+
kurt+
sp.ent+
time.ent+
entropy+
sfm+
meandom+
mindom+
maxdom+
dfrange+
modindx+
startdom+
enddom+
dfslope+
meanpeakf+
peakf+
maxslope+
minslope+
abs_minslope+
pos_slopes+
neg_slopes+
turns+
meanslope+
segments,
CV= F,
data=pre)
# get vocal distance as Mahalanobis distance between group centroids
pre_distance <- as.matrix(dist(dfa_pre$means %*% dfa_pre$scaling))
# get all POST-INTRO dyadic distances
post <- rbind(tole_post,lp)
# classify calls to bat using a single dfa without cross validation
dfa_post <- lda(individual ~
duration+
meanfreq+
sd+
freq.median+
freq.Q25+
freq.Q75+
freq.IQR+
time.median+
time.Q25+
time.Q75+
time.IQR+
skew+
kurt+
sp.ent+
time.ent+
entropy+
sfm+
meandom+
mindom+
maxdom+
dfrange+
modindx+
startdom+
enddom+
dfslope+
meanpeakf+
peakf+
maxslope+
minslope+
abs_minslope+
pos_slopes+
neg_slopes+
turns+
meanslope+
segments,
CV= F,
data=post)
# get vocal distance as Mahalanobis distance between group centroids
post_distance <- as.matrix(dist(dfa_post$means %*% dfa_post$scaling))
# reorder matrices
order <- c(unique(tole_pre$individual), unique(lp$individual))
pre_dist <- reorder_mat(pre_distance, order)
post_dist <- reorder_mat(post_distance, order)
library(graph4lg)
# reorder matrices
order <- c(unique(tole_pre$individual), unique(lp$individual))
pre_dist <- reorder_mat(pre_distance, order)
post_dist <- reorder_mat(post_distance, order)
# get call similarity before introduction
sim1 <- 1 - (pre_dist/max(pre_dist))
# get call similarity after introduction
sim2 <- 1 - (post_dist/max(post_dist))
# get increase in similarity
diff <- sim2 - sim1
#* make familiar vs introduced matrix ----
# 0 = originally familiar (same capture site)
# 1 = introduced (different capture site)
lp_bats <- unique(lp$individual)
tole_bats <- unique(tole_pre$individual)
a <- matrix(nrow = 12, ncol = 12)
col1 <- c(rep(0,times=7),rep(1,times=5))
col2 <- c(rep(1,times=7),rep(0,times=5))
v <- c(rep(col1,times=7),rep(col2,times=5))
a[,] <- v
diag(a) <- NA
rownames(a) <- c(tole_bats,lp_bats)
colnames(a) <- c(tole_bats,lp_bats)
# check names match
colnames(a) == colnames(diff)
#* Mantel test ----
set.seed(123)
mantel(diff, a, na.rm=T, method = "spearman")
library(vegan)
#* Mantel test ----
set.seed(123)
mantel(diff, a, na.rm=T, method = "spearman")
# get within-dyad convergence ----
dd <-
tibble(bat1=rownames(diff)[row(diff)],
bat2=colnames(diff)[col(diff)],
diff=c(diff)) %>%
filter(bat1 != bat2) %>%
mutate(site1= ifelse(bat1 %in% lp_bats, "lp", "tole")) %>%
mutate(site2= ifelse(bat2 %in% lp_bats, "lp", "tole")) %>%
mutate(dyad= ifelse(bat1<bat2, paste(bat1,bat2, sep="_"), paste(bat2,bat1, sep="_"))) %>%
mutate(introduced= site1!= site2) %>%
group_by(dyad, introduced) %>%
summarize(diff= mean(diff)) %>%
ungroup() %>%
separate(dyad, into = c("bat1", "bat2"))
# how many introduced dyads had increased similarity? ----
t <-
dd %>%
mutate(increase= diff>0) %>%
mutate(group= case_when(
introduced ~ "introduced",
bat1 %in% lp_bats & bat2 %in% lp_bats ~ "Las Pavas",
bat1 %in% tole_bats & bat2 %in% tole_bats ~ "Tole")) %>%
group_by(group) %>%
summarize(increase= sum(increase), n=n()) %>%
mutate(decrease = n-increase) %>%
dplyr::select(-n)
View(t)
# multi-membership model ----
library(brms)
fit <- brm(diff ~ introduced + (1|mm(bat1,bat2)),
family= gaussian(),
data= dd,
seed = 123,
iter = 6000,
warmup = 2000,
chains = 4)
setwd("/Users/jkvrtilek/Desktop/OSU/PhD/GitHub/call-convergence-2025")
# get data
# update this link after data are public
d <- read_csv('vocal_social_data.csv')
# load packages
library(tidyverse)
# get data
# update this link after data are public
d <- read_csv('vocal_social_data.csv')
View(d)
d <- read.csv("/Users/jkvrtilek/Desktop/OSU/PhD/GitHub/_LFS/call-convergence-2025/vampire_call_measures_transformed.csv")
View(d)
View(d)
d <- read.csv("/Users/jkvrtilek/Desktop/OSU/PhD/GitHub/call-convergence-2025/vocal-distance-94-bats.csv"))
d <- read.csv("/Users/jkvrtilek/Desktop/OSU/PhD/GitHub/call-convergence-2025/vocal-distance-94-bats.csv")
bats <- colnames(d)
meta <- read.csv("/Users/jkvrtilek/Desktop/OSU/PhD/Obsolete/Calls/whole_recordings_renamed_metadata.csv")
View(meta)
